{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **CAP fhL 2025: Basics of LLMs: Prompting, Function Calling & Structured Outputs**\n",
    "\n",
    "Welcome to this tutorial on working with Large Language Models (LLMs). We'll explore how to effectively use these powerful AI systems through the OpenAI API.\n",
    "\n",
    "### Background\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized how we interact with AI systems. There are three key concepts that make these models particularly powerful:\n",
    "\n",
    "1. **Prompting**: The art of effectively communicating with LLMs through carefully crafted inputs. This includes techniques like few-shot learning, chain-of-thought prompting, and system messages that help guide the model's behavior.\n",
    "\n",
    "2. **Function Calling**: A powerful capability that allows LLMs to interact with external tools and APIs. By defining functions that the model can \"call\", we can create applications that combine the model's intelligence with real-world actions.\n",
    "\n",
    "3. **Structured Outputs**: The ability to get responses in specific formats (JSON, XML, etc.) makes it easier to integrate LLMs into existing systems and ensure consistent, parseable outputs.\n",
    "\n",
    "Using the OpenAI API, we'll explore these concepts through practical examples and demonstrate how they can be combined to create powerful applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Getting Started**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the OpenAI client and submit a test request\n",
    "To setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage. \n",
    "\n",
    "You can get an API key by following these steps:\n",
    "1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n",
    "2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n",
    "3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)\n",
    "\n",
    "Once we have this setup, let's start with a simple {text} input to the model for our first request. We'll use both `system` and `user` messages for our first request, and we'll receive a response from the `assistant` role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the API key from the environment variable\n",
    "load_dotenv()\n",
    "\n",
    "## Set the API key and model name\n",
    "MODEL=\"gpt-4o-mini\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Of course! \\(2 + 2 = 4\\).\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Help me with my math homework!\"}, # <-- This is the system message that provides context to the model\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Could you solve 2+2?\"}  # <-- This is the user message for which the model will generate a response\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Prompting â€“ Crafting Effective Interactions**\n",
    "\n",
    "## **2.1 Basics of Prompting**  \n",
    "\n",
    "### **What is a Prompt?**  \n",
    "- A **prompt** is the input given to an LLM to guide its response.  \n",
    "- Can be a question, command, or structured instruction.  \n",
    "- The way you phrase a prompt affects the modelâ€™s output.\n",
    "\n",
    "\n",
    "### **Simple Example: Asking a Model a Direct Question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The capital of Canada is Ottawa.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Canada?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.2 Improving Responses with Better Prompts**\n",
    "\n",
    "## **Zero-shot vs. Few-shot Prompting**  \n",
    "\n",
    "### **Zero-shot Prompting**\n",
    "- Model answers without any examples\n",
    "- Direct questions or instructions\n",
    "- Simplest form of prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: 'Hello' in French is 'Bonjour'.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please help me with translation.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate 'Hello' into French.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Few-shot Prompting**\n",
    "- Providing examples to guide responses\n",
    "- Helps model understand patterns\n",
    "- Improves accuracy and relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Bonjour\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please help me with translation.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate 'Hello' into Spanish.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hola\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate 'Hello' into French.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chain of Thought Prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Chain of Thought?**\n",
    "- A prompting technique that encourages models to show their reasoning process\n",
    "- Breaks down complex problems into logical steps\n",
    "- Improves accuracy and transparency of responses\n",
    "### **Key Benefits:**\n",
    "- Better problem-solving for complex tasks\n",
    "- Easier verification of model's logic\n",
    "- Reduced errors in mathematical and logical reasoning\n",
    "- More reliable outputs for multi-step problems\n",
    "\n",
    "### **Common Applications:**\n",
    " - Mathematical problem solving\n",
    " - Logical deduction\n",
    " - Multi-step analysis\n",
    " - Complex decision making\n",
    " - Step-by-step explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Without Chain of Thought Prompting (Basic Prompt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Jack is 12 years old, Jill is 6 years old.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Jack is twice as old as Jill. Four years ago, Jack was three times as old as Jill. How old are they now? Please simply return the answer as a number, no other text or explanation.\"} # <-- This is the basic prompt asking just for the answer.\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **With Chain of Thought Prompting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To solve this problem, let's define the variables for Jack's and Jill's current ages. We will let:\n",
      "\n",
      "- \\( J \\) represent Jack's current age.\n",
      "- \\( j \\) represent Jill's current age.\n",
      "\n",
      "From the problem statement, we have two key pieces of information, which we can turn into equations.\n",
      "\n",
      "1. The first condition states that \"Jack is twice as old as Jill.\" This can be represented mathematically as:\n",
      "\n",
      "   \\[\n",
      "   J = 2j\n",
      "   \\]\n",
      "\n",
      "2. The second condition states that \"Four years ago, Jack was three times as old as Jill.\" We need to express their ages four years ago:\n",
      "\n",
      "   - Jack's age four years ago: \\( J - 4 \\)\n",
      "   - Jill's age four years ago: \\( j - 4 \\)\n",
      "\n",
      "   According to this condition, we can write the equation:\n",
      "\n",
      "   \\[\n",
      "   J - 4 = 3(j - 4)\n",
      "   \\]\n",
      "\n",
      "Now we'll simplify this second equation. Expanding the right side gives us:\n",
      "\n",
      "\\[\n",
      "J - 4 = 3j - 12\n",
      "\\]\n",
      "\n",
      "Next, we can isolate \\( J \\) by adding 4 to both sides:\n",
      "\n",
      "\\[\n",
      "J = 3j - 8\n",
      "\\]\n",
      "\n",
      "Now we have two equations:\n",
      "\n",
      "1. \\( J = 2j \\)\n",
      "2. \\( J = 3j - 8 \\)\n",
      "\n",
      "We can set these two expressions for \\( J \\) equal to each other:\n",
      "\n",
      "\\[\n",
      "2j = 3j - 8\n",
      "\\]\n",
      "\n",
      "Now, let's solve for \\( j \\). First, subtract \\( 2j \\) from both sides:\n",
      "\n",
      "\\[\n",
      "0 = j - 8\n",
      "\\]\n",
      "\n",
      "Adding 8 to both sides gives:\n",
      "\n",
      "\\[\n",
      "j = 8\n",
      "\\]\n",
      "\n",
      "Now that we have Jill's current age, we can find Jack's current age using the first equation:\n",
      "\n",
      "\\[\n",
      "J = 2j = 2 \\times 8 = 16\n",
      "\\]\n",
      "\n",
      "So, the current ages are:\n",
      "\n",
      "- Jill is \\( 8 \\) years old.\n",
      "- Jack is \\( 16 \\) years old.\n",
      "\n",
      "Let's verify our solution with the conditions of the problem:\n",
      "- Jack is twice Jill's age: \\( 16 = 2 \\times 8 \\) (True)\n",
      "- Four years ago, Jack was \\( 16 - 4 = 12 \\) and Jill was \\( 8 - 4 = 4 \\). Check if Jack was three times Jill's age four years ago: \\( 12 = 3 \\times 4 \\) (True)\n",
      "\n",
      "Thus, the current ages are correct. \n",
      "\n",
      "Jack is **16 years old** and Jill is **8 years old**.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Jack is twice as old as Jill. Four years ago, Jack was three times as old as Jill. How old are they now? Please show your reasoning step by step.\"} # <-- This is the chain of thought prompting.\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Function Calling â€“ Extending LLM Capabilities**\n",
    "\n",
    "## **3.1 What is Function Calling?**\n",
    "\n",
    "Function calling is a powerful feature that allows LLMs to interact with external tools and APIs in a structured way. Instead of just generating text, models can identify when to use specific functions and provide the necessary parameters in the correct format.\n",
    "\n",
    "### **Key Concepts:**\n",
    "- Functions are defined with clear specifications (name, parameters, descriptions)\n",
    "- The model decides when and how to use available functions\n",
    "- Function calls enable real-world actions and data retrieval\n",
    "\n",
    "### **Common Use Cases:**\n",
    "- Retrieving real-time data (weather, stocks, etc.)\n",
    "- Performing calculations\n",
    "- Querying databases\n",
    "- Making API calls\n",
    "- Processing and transforming data\n",
    "\n",
    "## **3.2 Defining Functions for LLMs**\n",
    "\n",
    "When working with the OpenAI API, functions are defined using a JSON schema that specifies:\n",
    "- Function name\n",
    "- Description\n",
    "- Required and optional parameters\n",
    "- Parameter types and constraints\n",
    "\n",
    "### **Example: Calling get_weather function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: [ChatCompletionMessageToolCall(id='call_hHA7jLVB4Sj95Y1nx267dzL5', function=Function(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather'), type='function')]\n",
      "\n",
      "get_weather_function called with Boston, MA and celsius\n",
      "Function response: The weather in Boston, MA is sunny and celsius.\n",
      "Messages after the function call:\n",
      "[{'role': 'system', 'content': 'You are a helpful weather assistant.'}, {'role': 'user', 'content': \"What's the weather like in Boston?\"}, {'role': 'assistant', 'content': None, 'tool_calls': [ChatCompletionMessageToolCall(id='call_hHA7jLVB4Sj95Y1nx267dzL5', function=Function(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather'), type='function')]}, {'role': 'tool', 'content': 'The weather in Boston, MA is sunny and celsius.', 'tool_call_id': 'call_hHA7jLVB4Sj95Y1nx267dzL5'}]\n",
      "The message will be sent to the model again with the function response.\n",
      "\n",
      "Assistant: The weather in Boston, MA is sunny. If you need more specific details like temperature or forecasts, please let me know!\n"
     ]
    }
   ],
   "source": [
    "# Define the weather function\n",
    "import json\n",
    "\n",
    "weather_function = {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_current_weather(location, unit):\n",
    "    # This is a mock implementation. In a real scenario, you'd make an API call here.\n",
    "    return f\"The weather in {location} is sunny and {unit}.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}\n",
    "]\n",
    "\n",
    "# Make a call to OpenAI with function calling\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,  # Use a model that supports function calling\n",
    "    messages=messages,\n",
    "    tools=[{\"type\": \"function\", \"function\": weather_function}],\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {response.choices[0].message.tool_calls}\\n\")\n",
    "\n",
    "# Add the assistant's response and tool calls to the messages\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content, \"tool_calls\": response.choices[0].message.tool_calls})\n",
    "\n",
    "if response.choices[0].message.tool_calls:\n",
    "    # Call the function with the provided arguments\n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "    function_name = tool_call.function.name\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    # Execute the function with the provided arguments\n",
    "    if function_name == \"get_current_weather\":\n",
    "        function_reponse = get_current_weather(function_args[\"location\"], function_args.get(\"unit\", \"celsius\"))\n",
    "        print(f\"get_weather_function called with {function_args['location']} and {function_args.get('unit', 'celsius')}\")\n",
    "        print(f\"Function response: {function_reponse}\")\n",
    "    else:\n",
    "        function_reponse = \"Function not found\"\n",
    "        print(f\"Function {function_name} not found\")\n",
    "    \n",
    "    # Add the function response to the messages\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": function_reponse,\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    })\n",
    "    \n",
    "print(\"Messages after the function call:\")\n",
    "print(messages)\n",
    "print(\"The message will be sent to the model again with the function response.\")\n",
    "\n",
    "# Make a call to OpenAI with function response\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL, # Use a model that supports function calling\n",
    "    messages=messages,\n",
    "    tools=[{\"type\": \"function\", \"function\": weather_function}],\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"\\nAssistant: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Structured Outputs â€“ Ensuring Consistency in Responses**\n",
    "\n",
    "## **4.1 Why Structured Outputs Matter**\n",
    "\n",
    "Structured outputs are crucial for building reliable applications with LLMs:\n",
    "\n",
    "### **Challenges with Free-text Outputs:**\n",
    "- Inconsistent formatting\n",
    "- Difficult to parse programmatically\n",
    "- Ambiguous or incomplete information\n",
    "- Hard to validate\n",
    "\n",
    "### **Benefits of Structured Outputs:**\n",
    "- Predictable format\n",
    "- Easy to parse and process\n",
    "- Validation possible\n",
    "- Seamless integration with other systems\n",
    "\n",
    "## **4.2 Getting JSON Outputs from LLMs**\n",
    "\n",
    "To get structured JSON outputs, we need to:\n",
    "1. Clearly specify the desired format in the system message\n",
    "2. Provide examples if needed\n",
    "3. Use response format parameters when available\n",
    "\n",
    "\n",
    "## **Example 1: Extracting Information From Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:\n",
      "{\n",
      "  \"event\": {\n",
      "    \"name\": \"AI Conference\",\n",
      "    \"date\": \"2025-07-15\",\n",
      "    \"time\": \"10:00 AM\",\n",
      "    \"location\": {\n",
      "      \"city\": \"Vancouver\",\n",
      "      \"country\": \"Canada\"\n",
      "    },\n",
      "    \"keynote_speaker\": \"Dr. Jane Smith\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "task = \"Extract event details from the following text and return them as JSON:\" + \\\n",
    "    \"Join us for the AI Conference on July 15, 2025, at 10:00 AM in Vancouver, Canada. The keynote speaker is Dr. Jane Smith.\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": task}\n",
    "  ],\n",
    "  response_format={\"type\": \"json_object\"} # Enforce JSON response\n",
    ")\n",
    "\n",
    "print(\"Assistant:\\n\" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example 2: Structured Outputs with Chain of Thought Prompting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw JSON Response ---\n",
      "{\n",
      "    \"reasoning\": \"Let's denote Jack's current age as J and Jill's current age as L. According to the problem, we have two key pieces of information: \\n\\n1. Jack is twice as old as Jill:  J = 2L  \\n2. Four years ago, Jack was three times as old as Jill: (J - 4) = 3(L - 4)  \\n\\nWe can substitute the first equation into the second.  \\nFrom J = 2L, we have:  \\n(J - 4) = 3(L - 4)  \\n(2L - 4) = 3(L - 4)  \\nNow, we need to expand and simplify:  \\n2L - 4 = 3L - 12  \\nNow, isolate L:  \\n2L - 3L = -12 + 4  \\n-L = -8  \\nL = 8  \\nSo Jill is currently 8 years old.  \\nNow, using the first equation to find Jack's age:  \\nJ = 2L = 2(8) = 16  \\nTherefore, Jack is currently 16 years old.  \\nWe can confirm: Four years ago, Jack was 12 and Jill was 4, and indeed 12 is three times 4.  \\nThus, the final ages are:\",\n",
      "    \"answer\": \"Jack is 16 years old and Jill is 8 years old.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define a complex problem that requires reasoning\n",
    "problem = \"Jack is twice as old as Jill. Four years ago, Jack was three times as old as Jill. How old are they now?\"\n",
    "\n",
    "# Define the JSON schema for structured output\n",
    "json_schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"reasoning\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Step-by-step explanation of how to solve the problem\"\n",
    "    },\n",
    "    \"answer\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The final answer to the problem\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"reasoning\", \"answer\"]\n",
    "}\n",
    "\n",
    "# Create a structured output request that combines chain-of-thought with JSON formatting\n",
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that solves problems step by step.\"},\n",
    "    {\"role\": \"user\", \"content\": problem}\n",
    "  ],\n",
    "  response_format={\n",
    "      \"type\": \"json_schema\", \n",
    "      \"json_schema\": {\n",
    "          \"name\": \"solve_problem\",\n",
    "          \"schema\": json_schema\n",
    "      }\n",
    "    },  # Enforce JSON response\n",
    ")\n",
    "\n",
    "# Parse and display the structured response\n",
    "response_json = json.loads(completion.choices[0].message.content)\n",
    "print(\"\\n--- Raw JSON Response ---\")\n",
    "print(json.dumps(response_json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Conclusion & Next Steps**  \n",
    "\n",
    "## **Recap of Key Takeaways**  \n",
    "Throughout this tutorial, we explored the fundamental techniques for effectively working with Large Language Models (LLMs) using the OpenAI API:  \n",
    "\n",
    "- **Prompting** â€“ Crafting effective inputs to guide the modelâ€™s behavior, including few-shot learning, chain-of-thought prompting, and system messages.  \n",
    "- **Function Calling** â€“ Enabling LLMs to interact with external APIs, tools, and databases by generating structured function calls.  \n",
    "- **Structured Outputs** â€“ Ensuring LLM responses are machine-readable (e.g., JSON, XML) for seamless integration into applications.  \n",
    "\n",
    "By combining these techniques, you can build **more reliable, interactive, and useful AI-powered applications.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Where to Go from Here: Advanced Topics**  \n",
    "\n",
    "Now that you understand the basics, here are some **advanced topics** to explore:  \n",
    "\n",
    "### **1. Memory in LLMs**  \n",
    "Persisting past interactions to create more personalized, context-aware AI responses.  \n",
    "- *Example:* Enabling a chatbot to remember user preferences across multiple conversations.  \n",
    "\n",
    "### **2. Retrieval-Augmented Generation (RAG)**  \n",
    "Enhancing LLMs with external knowledge sources for more factual and up-to-date answers.  \n",
    "- *Example:* Using a vector database like **FAISS** to retrieve relevant documents before generating a response.  \n",
    "\n",
    "### **3. Fine-Tuning Models**  \n",
    "Training LLMs on domain-specific data to improve performance in specialized tasks.  \n",
    "- *Example:* Fine-tuning an LLM on **legal documents** to generate more precise legal summaries.  \n",
    "\n",
    "### **4. AI Agents & Multi-Step Reasoning**  \n",
    "Using LLMs to plan and execute multi-step tasks, interacting with various APIs or tools.  \n",
    "- *Example:* An AI that can book flights, find hotels, and schedule events in a single workflow.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Resources for Further Learning**  \n",
    "\n",
    "To continue your learning journey, here are some **recommended resources**:  \n",
    "\n",
    "- **[Official OpenAI Docs](https://platform.openai.com/docs)**  \n",
    "- **[Teams AI Library](https://github.com/microsoft/teams-ai/)** - Build AI-powered chat bots in Teams.\n",
    "- **[Data Analyst Agent in Teams (Template)](https://github.com/microsoft/teams-agent-accelerator-samples/tree/main/js/data-analyst-agent)**\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps**  \n",
    "\n",
    "ðŸš€ **Experiment with these concepts, build your own AI-powered tools, and dive deeper into the world of LLMs!**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
